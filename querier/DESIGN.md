# CS50 TSE Querier

## Design Spec
According to the Querier Requirements Spec, the TSE Querier is a standalone program that reads the index file produced by the TSE Indexer, and page files produced by the TSE Querier, and answers search queries submitted via stdin.

### User interface
The querier's only interface with the user is on the command-line. There must be two arguments passed when starting the program.

```
querier pageDirectory indexFilename
```

While the program is running, we shall textually prompt the user to type in a query. We satisfy it by printing out a list of documents that match the query. Then, we prompt again and so forth.

### Inputs and outputs
**Input**: the querier loads an index in-memory from `indexFilename` to determine the document IDs that match the words in a query

the querier constructs file pathnames from `pageDirectory` to read the URL from each matching document

**Output**: we output a list of documents that match the query, which shall include information such as document ID, score, and URL

### Functional decomposition into modules
We anticipate the following modules or functions:

1. *main*, which parses arguments, query, and query tokens, and initializes other modules to respond to query
2. *processQuery*, which determines and scores the pages that match the query
3. *rankPages*, which ranks pages according to their score and prints them out

And some helper modules that provide data structures:
1. index, a module providing the data structure to represent the in-memory index, and functions to read and write index files
2. pagedir, a module providing functions to open webpage files in pageDirectory
3. word, a module providing a function to normalize a word
4. tokens, a module providing the data structure to represent the tokenization of a query, and functions to manipulate it
5. file, a module providing functions to manipulate/access files

### Pseudo code for logic/algorithmic flow
The querier will run as follows:
```
parse command-line arguments
load index
while query != EOF,
    prompt for query
    read query and parse it
    tokenize query and parse it
    call processQuery
    call rankPages
delete index
```

where processQuery:
```
initialize 'pages' counters that map docID to a score
traverse through each query token,
    initialize 'temp' counters that stores the counters intersection of an 'andsequence'
    set 'temp' to be current word's (token's) index counters
    go to next word
    while we are in an 'andsequence',
        intersect current word counters with 'temp'
    union 'temp' with 'pages 
```

where rankPages:
```
(let 'pages' be the counters that map docID to a score)
get the key (docID) in 'pages' with the highest count (score), along with its count
in 'pages', set key's count to 0
while key's (pulled) count is not 0,
    open corresponding page file and read the URL 
    print docID, score, and URL of page
    again, get key in pages with highest count, along with its count
    in 'pages', set key's score to 0
```

### Major data structures
A key data structure here will be the `tokens` that store the tokenization of the query. `tokens` can be (and will be) as simple as an array of strings whose length we keep track of. We provide functionality for getting a string at a certain index, setting a string, deleting `tokens`, creating a new `tokens`, and, most importantly, getting a `tokens` from tokenizing a query.

Another key data structure will be `counters` in the `index` that, for a given word, map from docID to #occurrences. We will have to figure out a way to intersect and union these according to the `Requirements` spec.

### Testing plan
*Integration testing*.
1. Test `querier` with various invalid arguments 1. no arguments, 2. one argument, 3. three or more arguments, 4. invalid pageDirectory (non-existent path) 5. invalid pageDirectory (not a crawler directory) 6. invalid indexFile (non-existent path) 7. invalid indexFile (read-only directory) 8. invalid indexFile (existing, write-only file)
2. Test `querier` with empty query and various invalid queries 1. operator in front 2. operator at the end 3. adjacent operators 4. non-alphabetic and non-whitespace character in query
3. Run `querier` on a variety of `pageDirectories` and their corresponding `indexFilename`. For each of these, run at least 10 queries generated by `fuzzquery` program, trying to make sure we get a decent-sized set of queries that include: 1. a query that doesn't match any documents 2. a single word 3. multiple words (some explicit/implicit combination of: ) 4. a conjunction 5. a disjunction 6. a disjunction of conjunctions 7. disjunctions of conjuctions
4. Run `valgrind` on `querier` to ensure no memory leaks or errors
